spring.application.name=kafka-consumer
server.port=8092

spring.kafka.consumer.bootstrapServers=192.168.229.13:9092,192.168.229.13:9093,192.168.229.13:9094


#spring.kafka.consumer.clientId="kafka-consumer"
spring.kafka.consumer.group-id=myGroup

#自动提交
spring.kafka.consumer.enableAutoCommit=true

#自动向zookeeper提交offset的频率，默认：5000
spring.kafka.consumer.autoCommitInterval=10

#0:READ_UNCOMMITTED, 1:READ_COMMITTED;
spring.kafka.consumer.isolationLevel=READ_COMMITTED

# 没有初始化的offset时，可以设置以下三种情况：（默认：latest）
# earliest
# 当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，从头开始消费
# latest
# 当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，消费新产生的该分区下的数据
# none
# topic各分区都存在已提交的offset时，从offset后开始消费；只要有一个分区不存在已提交的offset，则抛出异常
spring.kafka.consumer.autoOffsetReset=latest

#安全认证
#spring.kafka.consumer.ssl=

#Fetch请求发给broker后，在broker中可能会被阻塞的（当topic中records的总size小于fetch.min.bytes时），此时这个fetch请求耗时就会比较长。
#这个配置就是来配置consumer最多等待response多久。
spring.kafka.consumer.fetchMaxWait=100
#每次fetch请求时，server应该返回的最小字节数。如果没有足够的数据返回，请求会等待，直到足够的数据才会返回。默认：1
spring.kafka.consumer.fetchMinSize=10

#消费超时时间，大小不能超过session.timeout.ms，默认：3000
spring.kafka.consumer.heartbeatInterval=100

#key反序列化类，实现org.apache.kafka.common.serialization.Deserializer接口
spring.kafka.consumer.keyDeserializer=org.apache.kafka.common.serialization.StringDeserializer
#value反序列化类，实现org.apache.kafka.common.serialization.Deserializer接口
spring.kafka.consumer.valueDeserializer=org.apache.kafka.common.serialization.StringDeserializer

#max.poll.records条数据需要在session.timeout.ms这个时间内处理完，默认：500
spring.kafka.consumer.maxPollRecords=500

